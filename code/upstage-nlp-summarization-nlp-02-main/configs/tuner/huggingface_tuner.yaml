_target_: src.tuners.huggingface_tuner.HuggingFaceTuner
hparams:
  pretrained_model_name:
    - MLP-KTLim/llama-3-Korean-Bllossom-8B
  lr:
    low: 0.00001
    high: 0.00005
    log: False
  t_max:
    low: 25
    high: 100
    log: False
  eta_min:
    low: 0.0000025
    high: 0.00001
    log: False

module_params:
  model_execution_mode: ${mode}
  quantization_type: ${quantization_type}
  quantization_config:
    load_in_4bit: ${quantization_config.load_in_4bit}
    bnb_4bit_quant_type: ${quantization_config.bnb_4bit_quant_type}
    bnb_4bit_use_double_quant: ${quantization_config.bnb_4bit_use_double_quant}
    bnb_4bit_compute_dtype: ${quantization_config.bnb_4bit_compute_dtype}
  peft_type: ${peft_type}
  peft_config:
    r: ${peft_config.r}
    lora_alpha: ${peft_config.lora_alpha}
    target_modules: ${peft_config.target_modules}
    lora_dropout: ${peft_config.lora_dropout}
    bias: ${peft_config.bias}
    task_type: ${peft_config.task_type}
  interval: step
  options: ${options}
  target_max_length: ${target_max_length}
  target_min_length: ${target_min_length}
  per_device_save_path: ${per_device_save_path}
  target_column_name: ${target_column_name}
  devices: ${devices}
  accelerator: ${accelerator}
  strategy: ${strategy}
  log_every_n_steps: ${log_every_n_steps}
  precision: ${precision}
  accumulate_grad_batches: ${accumulate_grad_batches}
  max_epochs: ${epoch}
  monitor: ${monitor}
  mode: ${tracking_direction}
  patience: ${patience}
  min_delta: ${min_delta}
  options: ${options}

num_trials: ${num_trials}
seed: ${seed}
hparams_save_path: ${hparams_save_path}