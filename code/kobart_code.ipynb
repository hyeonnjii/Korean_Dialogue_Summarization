{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7011' max='38950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7011/38950 1:26:21 < 6:33:33, 1.35 it/s, Epoch 9/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.251900</td>\n",
       "      <td>1.929083</td>\n",
       "      <td>0.178924</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.169936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>0.626462</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>0.103290</td>\n",
       "      <td>0.270257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.588959</td>\n",
       "      <td>0.302021</td>\n",
       "      <td>0.111796</td>\n",
       "      <td>0.284082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.570849</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.111625</td>\n",
       "      <td>0.285576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.564366</td>\n",
       "      <td>0.306859</td>\n",
       "      <td>0.118259</td>\n",
       "      <td>0.288737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.557852</td>\n",
       "      <td>0.315099</td>\n",
       "      <td>0.120230</td>\n",
       "      <td>0.298619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>0.316611</td>\n",
       "      <td>0.121394</td>\n",
       "      <td>0.299759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.559054</td>\n",
       "      <td>0.312789</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.295429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>0.315043</td>\n",
       "      <td>0.120615</td>\n",
       "      <td>0.298649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig, BartTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb\n",
    "\n",
    "# tokenizer ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./\",  # ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "        \"model_name\": \"digit82/kobart-summarization\",  # ëª¨ë¸ ì´ë¦„\n",
    "        \"output_dir\": \"./\"  # ì¶œë ¥ ê²½ë¡œ\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 100,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', \n",
    "                           '#PassportNumber#', \"#CarNumber#\", \"#DateOfBirth#\", \"#PassportNumber#\",\n",
    "                           \"#SSN#\", '#CardNumber#','#Email#','#Person#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 50,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"none\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"wandb_repo\",\n",
    "        \"project\": \"project_name\",\n",
    "        \"name\": \"run_name\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"/home/ubuntu/upstage-nlp\", \n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 25,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_beams\": 10,\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„± ì •ë³´ë¥¼ YAML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "config_path = \"/home/ubuntu/upstage-nlp/code/config.yaml\"\n",
    "with open(config_path, \"w\", encoding='utf-8') as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True, default_flow_style=False)\n",
    "\n",
    "# ì €ì¥ëœ config íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "with open(config_path, \"r\", encoding='utf-8') as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "    \n",
    "# (ì„ íƒ) ì´ê³³ì— ì‚¬ìš©ìê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "# loaded_config['wandb']['entity'] = \"hkkyu\"\n",
    "# loaded_config['wandb']['name'] = \"upstage-baseline-multimodel.v.2\"\n",
    "# loaded_config['wandb']['project'] = \"upstage-nlp\"\n",
    "# Train ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(os.path.join(loaded_config['general']['data_path'], '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/train.csv'))\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
    "class Preprocess:\n",
    "    def __init__(self, bos_token: str, eos_token: str) -> None:\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            df = df[['fname', 'dialogue', 'summary', 'topic']]\n",
    "            return df\n",
    "        else:\n",
    "            df = df[['fname', 'dialogue']]\n",
    "            return df\n",
    "\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n",
    "\n",
    "# Train Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, topics, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.topics = topics\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        item['topics'] = self.topics[idx]  # Add topics to item\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Validation Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, topics, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.topics = topics\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        item['topics'] = self.topics[idx]  # Add topics to item\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/train.csv')\n",
    "    val_file_path = os.path.join(data_path, '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/dev.csv')\n",
    "\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_outputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                                          add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_topics = train_data['topic'].tolist()\n",
    "    val_topics = val_data['topic'].tolist()\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_outputs, train_topics, len(encoder_input_train))\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'],\n",
    "\n",
    "                                             return_token_type_ids=False)\n",
    "    val_tokenized_decoder_outputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_outputs, val_topics, len(encoder_input_val))\n",
    "\n",
    "    return train_inputs_dataset, val_inputs_dataset\n",
    "\n",
    "# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
    "def compute_metrics(config, tokenizer, pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    replaced_predictions = [sentence.replace(token, \" \") for sentence in decoded_preds for token in remove_tokens]\n",
    "    replaced_labels = [sentence.replace(token, \" \") for sentence in labels for token in remove_tokens]\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result\n",
    "\n",
    "# í•™ìŠµì„ ìœ„í•œ Trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ ì •ì˜\n",
    "def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['general']['output_dir'],  # model output directory\n",
    "        overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "        learning_rate=config['training']['learning_rate'],  # learning rate\n",
    "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # batch size per device during training\n",
    "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],  # batch size for evaluation\n",
    "        warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "        lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "        optim=config['training']['optim'],\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "        evaluation_strategy=config['training']['evaluation_strategy'],  # evaluation strategy to adopt during training\n",
    "        save_strategy=config['training']['save_strategy'],\n",
    "        save_total_limit=config['training']['save_total_limit'],  # number of total save model\n",
    "        fp16=config['training']['fp16'],\n",
    "        load_best_model_at_end=config['training']['load_best_model_at_end'],  # load best model at the end\n",
    "        seed=config['training']['seed'],\n",
    "        logging_dir=config['training']['logging_dir'],  # directory for storing logs\n",
    "        logging_strategy=config['training']['logging_strategy'],\n",
    "        predict_with_generate=config['training']['predict_with_generate'],  # to use BLEU or ROUGE score\n",
    "        generation_max_length=config['training']['generation_max_length'],\n",
    "        do_train=config['training']['do_train'],\n",
    "        do_eval=config['training']['do_eval'],\n",
    "        report_to=config['training']['report_to']  # (optional) use wandb\n",
    "    )\n",
    "\n",
    "    # wandb.init(\n",
    "    #     entity=config['wandb']['entity'],\n",
    "    #     project=config['wandb']['project'],\n",
    "    #     name=config['wandb']['name']\n",
    "    # )\n",
    "\n",
    "    # os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "    # os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model,  # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=train_inputs_dataset,  # training dataset\n",
    "        eval_dataset=val_inputs_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "# í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    model_name = config['general']['model_name']\n",
    "    bart_config = BartConfig.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(config['general']['model_name'], config=bart_config)\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "\n",
    "    return generate_model, tokenizer\n",
    "\n",
    "def main(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n",
    "\n",
    "    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "    trainer.train()\n",
    "\n",
    "    #wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckt_path = loaded_config['inference']['ckt_path'] = '/home/ubuntu/upstage-nlp/competition-pipeline/checkpoint-7011'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.2.0+cu121\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Load tokenizer & model complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_data:\n",
      "#Person1#: ë”ìŠ¨ ì”¨, ë°›ì•„ì“°ê¸° ì¢€ í•´ì£¼ì„¸ìš”. \n",
      "#Person2#: ë„¤, ì‹¤ì¥ë‹˜...\n",
      "#Person1#: ì´ê²ƒì€ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ì—ê²Œ ë‚´ë¶€ ë©”ëª¨ë¡œ ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤€ë¹„ë˜ì…¨ë‚˜ìš”?\n",
      "#Person2#: ë„¤, ì‹¤ì¥ë‹˜. ì‹œì‘í•˜ì…”ë„ ë©ë‹ˆë‹¤.\n",
      "#Person1#: ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì£¼ì˜í•˜ë¼... ì¦‰ì‹œ íš¨ë ¥ì„ ë°œíœ˜í•˜ì—¬, ëª¨ë“  ì‚¬ë¬´ì‹¤ í†µì‹ ì€ ì´ë©”ì¼ í†µì‹ ê³¼ ê³µì‹ ë©”ëª¨ë¡œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì§ì›ë“¤ì´ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì—„ê²©íˆ ê¸ˆì§€ë©ë‹ˆë‹¤.\n",
      "#Person2#: ì‹¤ì¥ë‹˜, ì´ê²ƒì€ ë‚´ë¶€ í†µì‹ ì—ë§Œ ì ìš©ë˜ëŠ” ê±´ê°€ìš”? ì•„ë‹ˆë©´ ì™¸ë¶€ í†µì‹ ì—ë„ ì œí•œì´ ë˜ëŠ” ê±´ê°€ìš”?\n",
      "#Person1#: ì´ê²ƒì€ ëª¨ë“  í†µì‹ ì— ì ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤, ì´ ì‚¬ë¬´ì‹¤ ë‚´ì˜ ì§ì›ë“¤ ì‚¬ì´ë¿ë§Œ ì•„ë‹ˆë¼ ì™¸ë¶€ í†µì‹ ì—ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.\n",
      "#Person2#: í•˜ì§€ë§Œ ì‹¤ì¥ë‹˜, ë§ì€ ì§ì›ë“¤ì´ ê³ ê°ê³¼ ì†Œí†µí•˜ê¸° ìœ„í•´ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "#Person1#: ê·¸ë“¤ì€ ê·¸ë“¤ì˜ ì˜ì‚¬ì†Œí†µ ë°©ë²•ì„ ë°”ê¾¸ì–´ì•¼ë§Œ í•©ë‹ˆë‹¤. ì´ ì‚¬ë¬´ì‹¤ì—ì„œ ëˆ„êµ¬ë„ ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¥¼ ì›í•©ë‹ˆë‹¤. ë„ˆë¬´ ë§ì€ ì‹œê°„ì„ ë‚­ë¹„í•˜ê²Œ ë©ë‹ˆë‹¤! ì´ì œ, ë©”ëª¨ë¥¼ ê³„ì†í•´ì£¼ì„¸ìš”. ìš°ë¦¬ê°€ ì–´ë””ê¹Œì§€ í–ˆë‚˜ìš”?\n",
      "#Person2#: ì´ê²ƒì€ ë‚´ë¶€ì™€ ì™¸ë¶€ í†µì‹ ì— ì ìš©ë©ë‹ˆë‹¤.\n",
      "#Person1#: ê·¸ë ‡ìŠµë‹ˆë‹¤. ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ê³„ì† ì‚¬ìš©í•˜ëŠ” ì–´ë–¤ ì§ì›ì´ë¼ë„ ë¨¼ì € ê²½ê³ ë¥¼ ë°›ê³  ì§ë¬´ ì •ì§€ì— ì²˜í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ìœ„ë°˜ ì‹œì—ëŠ” ì§ì›ì€ í•´ê³ ì— ì²˜í•´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ì •ì±…ì— ëŒ€í•œ ì–´ë–¤ ì§ˆë¬¸ì´ë¼ë„ ë¶€ì„œì¥ì—ê²Œ ì§ì ‘ ë¬¸ì˜í•˜ë©´ ë©ë‹ˆë‹¤.\n",
      "#Person2#: ê·¸ê²Œ ë‹¤ì‹ ê°€ìš”?\n",
      "#Person1#: ë„¤. ì´ ë©”ëª¨ë¥¼ ì˜¤í›„ 4ì‹œ ì „ì— ëª¨ë“  ì§ì›ì—ê²Œ íƒ€ì´í•‘í•˜ì—¬ ë°°í¬í•´ ì£¼ì„¸ìš”.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:11<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "ckt_path = loaded_config['inference']['ckt_path'] = '/home/ubuntu/upstage-nlp/competition-pipeline/checkpoint-7011'\n",
    "# Inference Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'], '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-'*150)\n",
    "\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset\n",
    "\n",
    "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model, tokenizer\n",
    "\n",
    "def inference(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
    "\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            generated_ids = generate_model.generate(input_ids=item['input_ids'].to(device),\n",
    "                                                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                                                    early_stopping=config['inference']['early_stopping'],\n",
    "                                                    max_length=config['inference']['generate_max_length'],\n",
    "                                                    num_beams=config['inference']['num_beams'])\n",
    "            for ids in generated_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        \"fname\": test_data['fname'],\n",
    "        \"summary\": summary,\n",
    "    })\n",
    "\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output2.csv\"), index=False)\n",
    "\n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main(loaded_config)\n",
    "\n",
    "    # í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gyu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a0496b952899894b4ce2b71581ccbdae075fafcdd748dcb0073534c101f0c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
