{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7011' max='38950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7011/38950 1:26:21 < 6:33:33, 1.35 it/s, Epoch 9/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.251900</td>\n",
       "      <td>1.929083</td>\n",
       "      <td>0.178924</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.169936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>0.626462</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>0.103290</td>\n",
       "      <td>0.270257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.588959</td>\n",
       "      <td>0.302021</td>\n",
       "      <td>0.111796</td>\n",
       "      <td>0.284082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.570849</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.111625</td>\n",
       "      <td>0.285576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.564366</td>\n",
       "      <td>0.306859</td>\n",
       "      <td>0.118259</td>\n",
       "      <td>0.288737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.557852</td>\n",
       "      <td>0.315099</td>\n",
       "      <td>0.120230</td>\n",
       "      <td>0.298619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>0.316611</td>\n",
       "      <td>0.121394</td>\n",
       "      <td>0.299759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.559054</td>\n",
       "      <td>0.312789</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.295429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>0.315043</td>\n",
       "      <td>0.120615</td>\n",
       "      <td>0.298649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "/home/ubuntu/anaconda3/envs/gyu/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig, BartTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb\n",
    "\n",
    "# tokenizer 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./\",  # 모델 생성에 필요한 데이터 경로를 지정합니다.\n",
    "        \"model_name\": \"digit82/kobart-summarization\",  # 모델 이름\n",
    "        \"output_dir\": \"./\"  # 출력 경로\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 100,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', \n",
    "                           '#PassportNumber#', \"#CarNumber#\", \"#DateOfBirth#\", \"#PassportNumber#\",\n",
    "                           \"#SSN#\", '#CardNumber#','#Email#','#Person#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 50,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"none\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"wandb_repo\",\n",
    "        \"project\": \"project_name\",\n",
    "        \"name\": \"run_name\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"/home/ubuntu/upstage-nlp\", \n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 25,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_beams\": 10,\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 모델 구성 정보를 YAML 파일로 저장합니다.\n",
    "config_path = \"/home/ubuntu/upstage-nlp/code/config.yaml\"\n",
    "with open(config_path, \"w\", encoding='utf-8') as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True, default_flow_style=False)\n",
    "\n",
    "# 저장된 config 파일을 불러옵니다.\n",
    "with open(config_path, \"r\", encoding='utf-8') as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "    \n",
    "# (선택) 이곳에 사용자가 사용할 wandb config 설정\n",
    "# loaded_config['wandb']['entity'] = \"hkkyu\"\n",
    "# loaded_config['wandb']['name'] = \"upstage-baseline-multimodel.v.2\"\n",
    "# loaded_config['wandb']['project'] = \"upstage-nlp\"\n",
    "# Train 데이터 로드\n",
    "train_df = pd.read_csv(os.path.join(loaded_config['general']['data_path'], '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/train.csv'))\n",
    "\n",
    "# 데이터 전처리 클래스\n",
    "class Preprocess:\n",
    "    def __init__(self, bos_token: str, eos_token: str) -> None:\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            df = df[['fname', 'dialogue', 'summary', 'topic']]\n",
    "            return df\n",
    "        else:\n",
    "            df = df[['fname', 'dialogue']]\n",
    "            return df\n",
    "\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n",
    "\n",
    "# Train Dataset 클래스 정의\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, topics, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.topics = topics\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        item['topics'] = self.topics[idx]  # Add topics to item\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Validation Dataset 클래스 정의\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, topics, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.topics = topics\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2)\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        item['topics'] = self.topics[idx]  # Add topics to item\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# 학습 데이터 준비 함수\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/train.csv')\n",
    "    val_file_path = os.path.join(data_path, '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/dev.csv')\n",
    "\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                                         add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_outputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                                          add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_topics = train_data['topic'].tolist()\n",
    "    val_topics = val_data['topic'].tolist()\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_outputs, train_topics, len(encoder_input_train))\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                                             add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'],\n",
    "\n",
    "                                             return_token_type_ids=False)\n",
    "    val_tokenized_decoder_outputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_outputs, val_topics, len(encoder_input_val))\n",
    "\n",
    "    return train_inputs_dataset, val_inputs_dataset\n",
    "\n",
    "# 모델 성능 평가 함수\n",
    "def compute_metrics(config, tokenizer, pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    replaced_predictions = [sentence.replace(token, \" \") for sentence in decoded_preds for token in remove_tokens]\n",
    "    replaced_labels = [sentence.replace(token, \" \") for sentence in labels for token in remove_tokens]\n",
    "\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result\n",
    "\n",
    "# 학습을 위한 Trainer 클래스와 매개변수 정의\n",
    "def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['general']['output_dir'],  # model output directory\n",
    "        overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "        learning_rate=config['training']['learning_rate'],  # learning rate\n",
    "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # batch size per device during training\n",
    "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],  # batch size for evaluation\n",
    "        warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "        lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "        optim=config['training']['optim'],\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "        evaluation_strategy=config['training']['evaluation_strategy'],  # evaluation strategy to adopt during training\n",
    "        save_strategy=config['training']['save_strategy'],\n",
    "        save_total_limit=config['training']['save_total_limit'],  # number of total save model\n",
    "        fp16=config['training']['fp16'],\n",
    "        load_best_model_at_end=config['training']['load_best_model_at_end'],  # load best model at the end\n",
    "        seed=config['training']['seed'],\n",
    "        logging_dir=config['training']['logging_dir'],  # directory for storing logs\n",
    "        logging_strategy=config['training']['logging_strategy'],\n",
    "        predict_with_generate=config['training']['predict_with_generate'],  # to use BLEU or ROUGE score\n",
    "        generation_max_length=config['training']['generation_max_length'],\n",
    "        do_train=config['training']['do_train'],\n",
    "        do_eval=config['training']['do_eval'],\n",
    "        report_to=config['training']['report_to']  # (optional) use wandb\n",
    "    )\n",
    "\n",
    "    # wandb.init(\n",
    "    #     entity=config['wandb']['entity'],\n",
    "    #     project=config['wandb']['project'],\n",
    "    #     name=config['wandb']['name']\n",
    "    # )\n",
    "\n",
    "    # os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "    # os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=train_inputs_dataset,  # training dataset\n",
    "        eval_dataset=val_inputs_dataset,  # evaluation dataset\n",
    "        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "# 학습을 위한 tokenizer와 사전 학습된 모델 로드\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    model_name = config['general']['model_name']\n",
    "    bart_config = BartConfig.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(config['general']['model_name'], config=bart_config)\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "\n",
    "    return generate_model, tokenizer\n",
    "\n",
    "def main(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n",
    "\n",
    "    trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)\n",
    "    trainer.train()\n",
    "\n",
    "    #wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckt_path = loaded_config['inference']['ckt_path'] = '/home/ubuntu/upstage-nlp/competition-pipeline/checkpoint-7011'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.2.0+cu121\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Load tokenizer & model complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_data:\n",
      "#Person1#: 더슨 씨, 받아쓰기 좀 해주세요. \n",
      "#Person2#: 네, 실장님...\n",
      "#Person1#: 이것은 오늘 오후까지 모든 직원에게 내부 메모로 전달되어야 합니다. 준비되셨나요?\n",
      "#Person2#: 네, 실장님. 시작하셔도 됩니다.\n",
      "#Person1#: 모든 직원들에게 주의하라... 즉시 효력을 발휘하여, 모든 사무실 통신은 이메일 통신과 공식 메모로 제한됩니다. 근무 시간 동안 직원들이 즉시 메시지 프로그램을 사용하는 것은 엄격히 금지됩니다.\n",
      "#Person2#: 실장님, 이것은 내부 통신에만 적용되는 건가요? 아니면 외부 통신에도 제한이 되는 건가요?\n",
      "#Person1#: 이것은 모든 통신에 적용되어야 합니다, 이 사무실 내의 직원들 사이뿐만 아니라 외부 통신에도 마찬가지입니다.\n",
      "#Person2#: 하지만 실장님, 많은 직원들이 고객과 소통하기 위해 즉시 메시지를 사용하고 있습니다.\n",
      "#Person1#: 그들은 그들의 의사소통 방법을 바꾸어야만 합니다. 이 사무실에서 누구도 즉시 메시지를 사용하지 않기를 원합니다. 너무 많은 시간을 낭비하게 됩니다! 이제, 메모를 계속해주세요. 우리가 어디까지 했나요?\n",
      "#Person2#: 이것은 내부와 외부 통신에 적용됩니다.\n",
      "#Person1#: 그렇습니다. 즉시 메시지를 계속 사용하는 어떤 직원이라도 먼저 경고를 받고 직무 정지에 처해질 것입니다. 두 번째 위반 시에는 직원은 해고에 처해질 것입니다. 이 새로운 정책에 대한 어떤 질문이라도 부서장에게 직접 문의하면 됩니다.\n",
      "#Person2#: 그게 다신가요?\n",
      "#Person1#: 네. 이 메모를 오후 4시 전에 모든 직원에게 타이핑하여 배포해 주세요.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:11<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "ckt_path = loaded_config['inference']['ckt_path'] = '/home/ubuntu/upstage-nlp/competition-pipeline/checkpoint-7011'\n",
    "# Inference Dataset 클래스 정의\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'], '/home/ubuntu/upstage-nlp/competition-pipeline/upstage-nlp-baseline/data/test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-'*150)\n",
    "\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                                              add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset\n",
    "\n",
    "# 추론을 위한 tokenizer와 학습된 모델을 불러옵니다.\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model, tokenizer\n",
    "\n",
    "def inference(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
    "\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            generated_ids = generate_model.generate(input_ids=item['input_ids'].to(device),\n",
    "                                                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                                                    early_stopping=config['inference']['early_stopping'],\n",
    "                                                    max_length=config['inference']['generate_max_length'],\n",
    "                                                    num_beams=config['inference']['num_beams'])\n",
    "            for ids in generated_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        \"fname\": test_data['fname'],\n",
    "        \"summary\": summary,\n",
    "    })\n",
    "\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output2.csv\"), index=False)\n",
    "\n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main(loaded_config)\n",
    "\n",
    "    # 학습된 모델의 test를 진행합니다.\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gyu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a0496b952899894b4ce2b71581ccbdae075fafcdd748dcb0073534c101f0c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
